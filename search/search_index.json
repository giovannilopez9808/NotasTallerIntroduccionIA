{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":""},{"location":"#bibliografia","title":"Bibliografia","text":"<ol> <li>Nocedal, J., &amp; Wright, S. J. (1999). Numerical optimization. doi:10.1007/978-0-387-40065-5</li> </ol>"},{"location":"attention/","title":"Attention mechanism","text":""},{"location":"cnnrnn/","title":"Hybrid model CNN-RNN","text":""},{"location":"convolutional/","title":"Convolutional","text":""},{"location":"dimensionality_reduction/","title":"Dimensionality reduction","text":""},{"location":"gan/","title":"GAN's","text":""},{"location":"gradient_descent/","title":"Gradient descent","text":""},{"location":"linear_neural_network/","title":"Linear neural networks","text":""},{"location":"optimization/","title":"Optimizaci\u00f3n","text":""},{"location":"optimization/#derivada-parcial","title":"Derivada parcial","text":"<p>Calcular la derivada parcial de una funci\u00f3n que mapea un vector a un valor escalar (\\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)), corresponde a calcular la derivada de la funci\u00f3n respecto a esa variable asuminedo las dem\u00e1s variables son constantes. Esto es: $$ \\large \\frac{\\partial f(x_1,x_2,x_3,\\dots,x_i,\\dots,x_n)}{\\partial x_i} \\overset{def}{=}\\underset{h\\rightarrow 0}{lim}\\;\\; \\frac{f(x_1,x_2,x_3,\\dots,x_i+h,\\dots,x_n)-f(x_1,x_2,x_3,\\dots,x_i,\\dots,x_n)}{h} $$ lo cual, se puede escribir de la siguiente manera: $$ \\frac{\\partial f(x)}{\\partial x_i}  \\overset{def}{=}\\underset{h\\rightarrow 0}{lim}\\;\\; \\frac{f(x+he_i)-f(x)}{h} $$ donde \\(x\\) es un vector de dimensi\u00f3n \\(n\\) y \\(e_i\\) es un vector unitario, el cual tiene ceros en cada posici\u00f3n excepto en \\(i\\) donde se encuentra un uno.  </p>"},{"location":"optimization/#gradiente-y-jacobiano","title":"Gradiente y jacobiano","text":"<p>En matem\u00e1ticas, el gradiente es una generalizaci\u00f3n de la derivada, ya que la derivada de una funci\u00f3n esta definida para unicamente funciones de una variable. El gradiente es una funci\u00f3n de valor vectorial. El gradiente esta definido como: $$ \\large \\nabla_f(x) \\overset{def}{=} \\left( \\begin{matrix} \\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\frac{\\partial f(x)}{\\partial x_3} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{matrix} \\right) $$ Con ello, se puede observar que el operador gradiente lleva a una funci\u00f3n \\(n\\) variabas variables a un vector de dimensi\u00f3n \\(n\\) donde el elemento i-esimo es la derivada parcial de la funci\u00f3n con respecto a la variable \\(i\\). El operador del gradiente se escribe como \\(\\nabla \\{\\}\\) y el vector gradiente como \\(\\nabla_f\\) </p> <p>Por otro lado, si se tiene una funci\u00f3n que mapea de un espacio vectorial de dimensi\u00f3n \\(n\\) a un espacio de dimensi\u00f3n \\(m\\) \\((f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m)\\), el concepto de derivada no se encuetra definido con el operador gradiente. Es por ello que se necesita definir el concepto del operador jacobiano. La definici\u00f3n del jacobiano es la siguiente: Sea \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), una funci\u00f3n cuyas derivadas parciales de primer orden existen en todo \\(\\mathbb{R}^n\\) y denotemos como \\(f_1,f_2,f_3,\\dots,f_m\\) a sus componentes escalares. Se define la matriz jacobiana de \\(f\\) en un punto \\(x \\in \\mathbb{R}^n\\) como: $$ \\large J_f(x) = \\left( \\begin{matrix} \\frac{\\partial f_1(x)}{\\partial x_1} &amp;\\frac{\\partial f_1(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_1(x)}{\\partial x_n} \\\\ \\frac{\\partial f_2(x)}{\\partial x_1} &amp;\\frac{\\partial f_2(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_2(x)}{\\partial x_n} \\\\ \\frac{\\partial f_3(x)}{\\partial x_1} &amp;\\frac{\\partial f_3(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_3(x)}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\ \\frac{\\partial f_m(x)}{\\partial x_1} &amp;\\frac{\\partial f_m(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_m(x)}{\\partial x_n} \\\\ \\end{matrix} \\right) $$</p>"},{"location":"optimization/#hessiano","title":"Hessiano","text":"<p>La matriz hessiana o hessiano de un campo escalar es una matriz cuadrada de dimensi\u00f3n \\(n \\times n\\) que tiene como elementos las derivadas parciales de segundo orden. La matriz hessiana se define como:  </p> <p>Sea \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), un campo escalar cuyas derivadas parciales de segundo grado existen (clase \\(C^2\\)). La matriz hessiana de \\(f\\), denotada por \\(H_f(x)\\), es una matriz cuadrada \\(n \\times n\\) definida como: $$ \\large H_f(x) = \\left( \\begin{matrix} \\frac{\\partial^2 f}{\\partial x_1^2} (x) &amp;\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} (x) &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} (x) \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} (x) &amp;\\frac{\\partial^2 f}{\\partial x_2^2} (x) &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} (x) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} (x) &amp;\\frac{\\partial^2 f}{\\partial x_n \\partial x_2} (x) &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} (x) \\\\ \\end{matrix} \\right) $$ Con ello, se afirma que el hessiano es aplicar el operador jacobiano al gradiente de la funci\u00f3n \\(f\\). $$ H_f(x) = J\\lbrace \\nabla f(x)\\rbrace = J\\lbrace J_f^T \\rbrace $$ Por otro lado, en algunos casos es m\u00e1s preferible calcular el hessiano por medio del operador gradiente. Para lograr este objetivo se define la operaci\u00f3n divergencia el cual es la siguiente: $$ \\nabla \\cdot f \\overset{def}{=} 1^T\\nabla f $$ Con esto se tiene que el hessiano es calculado como: $$ \\nabla^2 f = \\nabla^T \\nabla f $$</p>"},{"location":"optimization/#convexidad","title":"Convexidad","text":""},{"location":"optimization/#conjunto-convexo","title":"Conjunto convexo","text":"<p>Un conjunto $\\Omega \\subseteq \\mathbb{R}^n $ es convexo si $$ \\lambda x +(1-\\lambda)y \\in \\Omega, \\forall x,y \\in \\Omega\\;\\text{y}\\; \\lambda\\in [0,1] $$</p>"},{"location":"optimization/#funcion-convexa","title":"Funci\u00f3n convexa","text":"<p>Una funci\u00f3n \\(f: D\\rightarrow\\mathbb{R}\\) es estrictamente convexa si su dominio \\(D\\subset \\mathbb{R}^n\\) es convexo, y para todo \\(x,y \\in D\\) para todo \\(\\lambda \\in [0,1]\\), se verifica $$ f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) (1-\\lambda) f(x) $$ Se dice que una funci\u00f3n es convexa cuando no se requiere la igualdad de los terminos. Algunos ejemplos de funciones convexas son: $$ \\begin{aligned} f(x)&amp;=x^2 &amp; \\text{Funci\u00f3n cuadratica}\\nonumber \\\\ f(x)&amp;=Ax+b &amp; \\text{Funciones afines} \\nonumber\\\\ f(x)&amp;=||x|| &amp; \\text{Cualquier norma de un vextor} \\nonumber\\\\ \\end{aligned} \\nonumber $$</p>"},{"location":"optimization/#funciones-convexas-diferenciales","title":"Funciones convexas diferenciales","text":"<p>Sea \\(f: D\\rightarrow \\mathbb{R}\\) diferenciable sobre un dominio convexo y abierto \\(D\\). Entonces, \\(f\\) es convexa si y solo si $$ f(x) \\geq f(\\x')+\\nabla f(\\x')^T (x-\\x') $$ para todo \\(x,\\x' \\in D\\).</p>"},{"location":"optimization/#funciones-convexas-diferenciables-dos-veces","title":"Funciones convexas diferenciables dos veces","text":"<p>Sea \\(f: D\\rightarrow \\mathbb{R}\\) de clase \\(C^2\\) sobre un dominio convexo y abierto \\(D\\). Entonces, \\(f\\) es convexa si y solo si \\(H_f(x)\\) es semidefinida positiva para todo \\(x\\in D\\). Una matriz semidefinida positiva es una matriz hermitiana \\(M\\) cuando se cumple una de las siguientes condiciones:</p> <ol> <li>Para todos los vectores no nulos (\\(x\\in \\mathbb{R}^n\\)) tenemos que: $$ x^TMx \\geq 0 $$</li> <li>Todos los eigenvalores de \\(M\\) son positivos.</li> <li>La funci\u00f3n $$ &lt; x,y  &gt; = x^TMy $$ define un producto interno en \\(\\mathbb{R}^n\\).</li> <li>Todos los menores principales de \\(M\\) son positivos.</li> </ol>"},{"location":"optimization/#optimalidad","title":"Optimalidad","text":"<p>La optimizaci\u00f3n convexa trata el problema general de minimizar una funci\u00f3n convexa, sobre un conjunto factible tambi\u00e9n convexo: $$ \\underset{x\\in D}{min}\\; f(x) $$ donde \\(f:D\\rightarrow \\mathbb{R}\\) es convexa y $ D\\subset \\mathbb{R}^n$ es convexo. En un problema convexo no hay distinci\u00f3n entre m\u00ednimos globales y locales. Es por ello que existen diferentes clasificaciones dependiendo de las condiciones que cumplan. Estas clasificaciones son la siguientes:</p> <ul> <li>\u00d3ptimo global estricto: $x^* \\in \\mathbb{R}^n $ es el \u00f3ptimo global estricto si \\(f(y)&gt;f(x^*), \\forall y \\in \\mathbb{R}^n\\).</li> <li>\u00d3ptimo global: $x^* \\in \\mathbb{R}^n $ es el \u00f3ptimo global si \\(f(y)\\geq f(x^*), \\forall y \\in \\mathbb{R}^n\\).</li> <li>\u00d3ptimo Local Estricto: $x^* \\in \\mathbb{R}^n $ es el \u00f3ptimo local estricto si \\(f(y)&gt; f(x^*), \\forall y \\in \\Omega\\), donde \\(\\Omega\\) es una vecindad abierta que contiene a \\(x^*\\).</li> <li>\u00d3ptimo Local: \\(x^* \\in \\mathbb{R}^n\\) es el \u00f3ptimo local estricto si \\(f(y) \\geq f(x^*), \\forall y \\in \\Omega\\), donde \\(\\Omega\\) es una vecindad abierta que contiene a \\(x^*\\).</li> </ul>"},{"location":"optimization/#minimos-globales-bajo-convexidad","title":"Minimos globales bajo convexidad","text":"<p>Considerando el problema de optimizaci\u00f3n donde se supone que la funci\u00f3n \\(f\\) es diferenciable. Un punto \\(x^* \\in D\\) es m\u00ednimo global si y solo si $$ \\nabla f(x^*)^T (x-x^*) \\geq 0 \\;\\;\\; \\forall x \\in D $$</p>"},{"location":"optimization/#direccion-de-descenso","title":"Direcci\u00f3n de descenso","text":"<p>Se dice que \\(d\\) es una direcci\u00f3n de descenso en el punto \\(x'\\) para el problema propuesto de optimizaci\u00f3n cuando se cumple que: $$ f(x') &gt; f(x'+\\epsilon d) $$ donde \\(\\epsilon\\) es suficientemente peque\u00f1a. Con el fin de encontrar la condici\u00f3n de descenso se utilizara una expansi\u00f3n de la serie de Taylor de la funci\u00f3n \\(f\\) en el punto \\(x'\\). Del procedimiento se obtiene lo siguiente: $$ \\begin{aligned} f(x') &amp; &gt; f(x'+\\epsilon d)\\nonumber \\\\ &amp; &gt; f(x') + \\epsilon \\nabla f(x')^T d + \\epsilon^2 d^T \\nabla^2 f(x') d+ \\dots \\nonumber \\\\ &amp; \\approx f(x') + \\epsilon \\nabla f(x')^T d \\nonumber \\\\ \\end{aligned} $$ dado que \\(|\\epsilon^n| &gt; |epsilon|\\) para \\(n&gt;1\\). La direcci\u00f3n de descenso \\(d\\) debe cumplir que: $$ -\\nabla f(x')^T d &gt; 0 $$</p>"},{"location":"optimization/#condicion-de-optimalidad-de-primer-orden","title":"Condici\u00f3n de optimalidad de primer orden","text":"<p>Se tiene que en el punto \u00f3ptimo \\((x^*)\\) de una funci\u00f3n suave \\((f)\\) no existe una direcci\u00f3n de descenso. Esto es: $$ \\nexists d: -\\nabla f(x^*)^T d &gt; 0 $$</p> <p>ya que</p> \\[ \\nabla f(x^*) = 0 \\]"},{"location":"optimization/#condicion-de-optimalidad-de-segundo-orden","title":"Condici\u00f3n de optimalidad de segundo orden","text":"<p>En general, la condici\u00f3n de optimalidad de primer orden es necesaria, pero no es suficiente para encontrar una soluci\u00f3n. Es decir, en un m\u00ednimo local, el gradiente se desvanece. Sin embargo, esto tambi\u00e9n sucedee cuando se encuentra un m\u00e1ximo local o global. Para distinguir estos casos necesitamos la condiciones de segundo orden. Sea el problema de optimizaci\u00f3n sin restricciones: $$ \\underset{x\\in D}{min}\\; f(x) $$ La soluci\u00f3n \\((x^*)\\) satisface la condici\u00f3n de optimalidad primer orden: $$ \\nabla f(x^*) = 0 $$</p> <p>Esto evita que un m\u00e9todo de descenso actualice el punto \u00f3ptimo:</p> \\[ x' \\leftarrow x' - \\alpha \\nabla f(x^*) \\] <p>Sin embargo, por medio la approximaci\u00f3n de series de Taylor de segundo orden notamos que para un \\(\\epsilon\\) suficientemente peque\u00f1a:</p> \\[ f(x^*+\\epsilon d)  = f(x^*)+\\frac{1}{2}\\epsilon^2 d^T \\nabla^2 f(x^*) d \\] <p>como \\(x^*\\) es el punto \u00f3ptimo y \\(\\epsilon&gt;0\\) se obtiene que:</p> \\[ d^T \\nabla^2 f(x^*) d \\geq 0, \\forall d \\]"},{"location":"optimization/#condicion-suficiente-de-segundo-orden","title":"Condici\u00f3n suficiente de segundo orden","text":"<p>Sea el problema: $$ \\underset{x\\in D}{min}\\; f(x) $$</p> <p>con \\(f\\) suave y \\(x^*\\) es un punto tal que:</p> \\[ \\begin{aligned} \\nonumber \\nabla f(x^*) &amp;= 0 \\\\ \\nonumber \\nabla^2 f(x^*) &amp; \\;\\text{es definido positivo} \\\\ \\end{aligned} \\] <p>Con el fin de demostrar que \\(x^*\\) es un punto \u00f3ptimo se elije estudiarlo bajo la perspectiva que este punto se encuentra dentro de una bola abierta de radio \\(r\\). esto es: $$ D = {z:||x^*-x||&lt;0} $$</p> <p>tal que \\(\\nabla^2 f(x)\\) se mantenga definido positivo. Tomando un vector \\(d\\) tal que:</p> \\[ x+\\epsilon d \\in D \\] <p>Entonces, tomando un \\(\\epsilon\\) sumamente peque\u00f1o, se tiene que:</p> \\[ f(x^*+\\epsilon d)  = f(x^*)+\\frac{1}{2}\\epsilon^2 d^T \\nabla^2 f(x^*) d \\] <p>dado que:</p> \\[ \\begin{aligned} \\nonumber \\nabla f(x^*) = 0 \\\\ \\nonumber \\epsilon^2d^T\\nabla^2 f(x^*)d &gt; 0 \\\\ \\end{aligned} \\] <p>tenemos que</p> \\[ f(x^*+\\epsilon d) &gt; f(x^*) \\] <p>por lo tanto \\(x^*\\) es un m\u00ednimo de la funci\u00f3n.</p> <pre><code>from matplotlib import pyplot\nfrom numpy import (\n    concatenate,\n    arange,\n    zeros,\n)\n\nN = 10\nx = (arange(0, N)/N)**2\nz = zeros(int(N/2))\nf1 = concatenate([x[::-1], z, x])\nf2 = concatenate([-x[::-1], z, x])\nf3 = concatenate([x[::-1], x])\nfig, (ax1, ax2, ax3) = pyplot.subplots(\n    1,\n    3,\n    figsize=(20, 4)\n)\nax1.plot(f3, 'b')\nax1.annotate(\n    r'$\\nabla f=0, \\;\\; \\nabla^2f \\succ 0$',\n    xy=(9.5, 0),\n    xytext=(6.4, .2),\n    arrowprops=dict(\n        facecolor='red',\n        shrink=1\n    ),\n)\nax1.axis(\"off\")\nax2.plot(\n    f1,\n    'b'\n)\nax2.annotate(\n    r'$\\nabla f=0, \\;\\; \\nabla^2f = 0$',\n    xy=(12, 0), xytext=(8, .15),\n    arrowprops=dict(\n        facecolor='red',\n        shrink=1\n    ),\n)\nax2.axis(\"off\")\nax3.plot(f2, 'b')\nax3.annotate(\n    r'$\\nabla f=0, \\;\\; \\nabla^2f = 0$',\n    xy=(12, 0),\n    xytext=(8, .3),\n    arrowprops=dict(\n        facecolor='red',\n        shrink=1\n    ),\n)\nax3.axis(\"off\")\npyplot.tight_layout()\npyplot.savefig(\n    \"test.png\"\n)\n</code></pre> <p>De los tres casos mostrados en la figura anterior, los dos promeros (de la izquierda) corresponden a m\u00ednimos locales (soluciones) y el tercero a un punto de mesa (no-soluci\u00f3n).</p>"},{"location":"perceptron/","title":"Perceptron","text":""},{"location":"recurrent/","title":"Recurrent","text":""},{"location":"regression/","title":"Regression","text":""},{"location":"transformer/","title":"Transformers","text":""}]}