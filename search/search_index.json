{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":""},{"location":"attention/","title":"Attention mechanism","text":""},{"location":"cnnrnn/","title":"Hybrid model CNN-RNN","text":""},{"location":"convolutional/","title":"Convolutional","text":""},{"location":"dimensionality_reduction/","title":"Dimensionality reduction","text":""},{"location":"gan/","title":"GAN's","text":""},{"location":"gradient_descent/","title":"Gradient descent","text":""},{"location":"linear_neural_network/","title":"Linear neural networks","text":""},{"location":"optimization/","title":"Optimizaci\u00f3n","text":""},{"location":"optimization/#derivada-parcial","title":"Derivada parcial","text":"<p>Calcular la derivada parcial de una funci\u00f3n que mapea un vector a un valor escalar (\\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)), corresponde a calcular la derivada de la funci\u00f3n respecto a esa variable asuminedo las dem\u00e1s variables son constantes. Esto es: $$ \\large \\frac{\\partial f(x_1,x_2,x_3,\\dots,x_i,\\dots,x_n)}{\\partial x_i} \\overset{def}{=}\\underset{h\\rightarrow 0}{lim}\\;\\; \\frac{f(x_1,x_2,x_3,\\dots,x_i+h,\\dots,x_n)-f(x_1,x_2,x_3,\\dots,x_i,\\dots,x_n)}{h} $$ lo cual, se puede escribir de la siguiente manera: $$ \\frac{\\partial f(x)}{\\partial x_i}  \\overset{def}{=}\\underset{h\\rightarrow 0}{lim}\\;\\; \\frac{f(x+he_i)-f(x)}{h} $$ donde \\(x\\) es un vector de dimensi\u00f3n \\(n\\) y \\(e_i\\) es un vector unitario, el cual tiene ceros en cada posici\u00f3n excepto en \\(i\\) donde se encuentra un uno.  </p>"},{"location":"optimization/#gradiente-y-jacobiano","title":"Gradiente y jacobiano","text":"<p>En matem\u00e1ticas, el gradiente es una generalizaci\u00f3n de la derivada, ya que la derivada de una funci\u00f3n esta definida para unicamente funciones de una variable. El gradiente es una funci\u00f3n de valor vectorial. El gradiente esta definido como: $$ \\large \\nabla_f(x) \\overset{def}{=} \\left( \\begin{matrix} \\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\frac{\\partial f(x)}{\\partial x_3} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{matrix} \\right) $$ Con ello, se puede observar que el operador gradiente lleva a una funci\u00f3n \\(n\\) variabas variables a un vector de dimensi\u00f3n \\(n\\) donde el elemento i-esimo es la derivada parcial de la funci\u00f3n con respecto a la variable \\(i\\). El operador del gradiente se escribe como \\(\\nabla \\{\\}\\) y el vector gradiente como \\(\\nabla_f\\) </p> <p>Por otro lado, si se tiene una funci\u00f3n que mapea de un espacio vectorial de dimensi\u00f3n \\(n\\) a un espacio de dimensi\u00f3n \\(m\\) \\((f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m)\\), el concepto de derivada no se encuetra definido con el operador gradiente. Es por ello que se necesita definir el concepto del operador jacobiano. La definici\u00f3n del jacobiano es la siguiente: Sea \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), una funci\u00f3n cuyas derivadas parciales de primer orden existen en todo \\(\\mathbb{R}^n\\) y denotemos como \\(f_1,f_2,f_3,\\dots,f_m\\) a sus componentes escalares. Se define la matriz jacobiana de \\(f\\) en un punto \\(x \\in \\mathbb{R}^n\\) como: $$ \\large J_f(x) = \\left( \\begin{matrix} \\frac{\\partial f_1(x)}{\\partial x_1} &amp;\\frac{\\partial f_1(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_1(x)}{\\partial x_n} \\\\ \\frac{\\partial f_2(x)}{\\partial x_1} &amp;\\frac{\\partial f_2(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_2(x)}{\\partial x_n} \\\\ \\frac{\\partial f_3(x)}{\\partial x_1} &amp;\\frac{\\partial f_3(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_3(x)}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\ \\frac{\\partial f_m(x)}{\\partial x_1} &amp;\\frac{\\partial f_m(x)}{\\partial x_2} &amp; \\dots &amp;\\frac{\\partial f_m(x)}{\\partial x_n} \\\\ \\end{matrix} \\right) $$</p>"},{"location":"optimization/#hessiano","title":"Hessiano","text":"<p>La matriz hessiana o hessiano de un campo escalar es una matriz cuadrada de dimensi\u00f3n \\(n \\times n\\) que tiene como elementos las derivadas parciales de segundo orden. La matriz hessiana se define como:  </p> <p>Sea \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), un campo escalar cuyas derivadas parciales de segundo grado existen (clase \\(C^2\\)). La matriz hessiana de \\(f\\), denotada por \\(H_f(x)\\), es una matriz cuadrada \\(n \\times n\\) definida como: $$ \\large H_f(x) = \\left( \\begin{matrix} \\frac{\\partial^2 f}{\\partial x_1^2} (x) &amp;\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} (x) &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} (x) \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} (x) &amp;\\frac{\\partial^2 f}{\\partial x_2^2} (x) &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} (x) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\  \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} (x) &amp;\\frac{\\partial^2 f}{\\partial x_n \\partial x_2} (x) &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} (x) \\\\ \\end{matrix} \\right) $$ Con ello, se afirma que el hessiano es aplicar el operador jacobiano al gradiente de la funci\u00f3n \\(f\\). $$ H_f(x) = J\\lbrace \\nabla f(x)\\rbrace = J\\lbrace J_f^T \\rbrace $$ Por otro lado, en algunos casos es m\u00e1s preferible calcular el hessiano por medio del operador gradiente. Para lograr este objetivo se define la operaci\u00f3n divergencia el cual es la siguiente: $$ \\nabla \\cdot f \\overset{def}{=} 1^T\\nabla f $$ Con esto se tiene que el hessiano es calculado como: $$ \\nabla^2 f = \\nabla^T \\nabla f $$</p>"},{"location":"optimization/#optimizacion-convexa","title":"Optimizaci\u00f3n convexa","text":""},{"location":"optimization/#optimalidad","title":"Optimalidad","text":""},{"location":"optimization/#direccion-de-descenso","title":"Direcci\u00f3n de descenso","text":""},{"location":"optimization/#condicion-de-optimalidad-de-primer-orden","title":"Condici\u00f3n de optimalidad de primer orden","text":""},{"location":"optimization/#condicion-de-optimalidad-de-segundo-orden","title":"Condici\u00f3n de optimalidad de segundo orden","text":""},{"location":"optimization/#condiciones-de-segundo-orden-sin-restricciones","title":"Condiciones de segundo orden sin restricciones","text":""},{"location":"optimization/#condicion-necesaria-de-segundo-orden","title":"Condici\u00f3n necesaria de segundo orden","text":""},{"location":"optimization/#condicion-suficiente-de-segundo-orden","title":"Condici\u00f3n suficiente de segundo orden","text":""},{"location":"perceptron/","title":"Perceptron","text":""},{"location":"recurrent/","title":"Recurrent","text":""},{"location":"regression/","title":"Regression","text":""},{"location":"transformer/","title":"Transformers","text":""}]}