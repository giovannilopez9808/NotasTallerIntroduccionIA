
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../gradient_descent/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.13">
    
    
      
        <title>Optimización - Introducción a la inteligencia artificial</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../stylesheets/monokai.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#optimizacion" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Introducción a la inteligencia artificial" class="md-header__button md-logo" aria-label="Introducción a la inteligencia artificial" data-md-component="logo">
      
  <img src="../assets/images/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Introducción a la inteligencia artificial
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Optimización
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Introducción a la inteligencia artificial" class="md-nav__button md-logo" aria-label="Introducción a la inteligencia artificial" data-md-component="logo">
      
  <img src="../assets/images/icon.png" alt="logo">

    </a>
    Introducción a la inteligencia artificial
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introducción
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Optimización
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Optimización
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#derivada-parcial" class="md-nav__link">
    <span class="md-ellipsis">
      Derivada parcial
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradiente-y-jacobiano" class="md-nav__link">
    <span class="md-ellipsis">
      Gradiente y jacobiano
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hessiano" class="md-nav__link">
    <span class="md-ellipsis">
      Hessiano
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convexidad" class="md-nav__link">
    <span class="md-ellipsis">
      Convexidad
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimalidad" class="md-nav__link">
    <span class="md-ellipsis">
      Optimalidad
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradient_descent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tamaños de paso
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regresiónn lineal
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../dimensionality_reduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reducción de dimensionalidad
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../linear_neural_network/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Redes neuronales lineales
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../perceptron/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Perceptron multicapa
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convolutional/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Redes convolucionales
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../recurrent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Redes recurrentes
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cnnrnn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Redes híbridas
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mecanismo de atención
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../gan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN's
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="optimizacion"><strong>Optimización</strong></h1>
<h2 id="derivada-parcial"><strong>Derivada parcial</strong></h2>
<p>Calcular la derivada parcial de una función que mapea un vector a un valor escalar (<span class="arithmatex">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>), corresponde a calcular la derivada de la función respecto a esa variable asuminedo las demás variables son constantes. Esto es:
$$
\large
\frac{\partial f(x_1,x_2,x_3,\dots,x_i,\dots,x_n)}{\partial x_i} \overset{def}{=}\underset{h\rightarrow 0}{lim}\;\; \frac{f(x_1,x_2,x_3,\dots,x_i+h,\dots,x_n)-f(x_1,x_2,x_3,\dots,x_i,\dots,x_n)}{h}
$$
lo cual, se puede escribir de la siguiente manera:
$$
\frac{\partial f(x)}{\partial x_i}  \overset{def}{=}\underset{h\rightarrow 0}{lim}\;\; \frac{f(x+he_i)-f(x)}{h}
$$
donde <span class="arithmatex">\(x\)</span> es un vector de dimensión <span class="arithmatex">\(n\)</span> y <span class="arithmatex">\(e_i\)</span> es un vector unitario, el cual tiene ceros en cada posición excepto en <span class="arithmatex">\(i\)</span> donde se encuentra un uno.  </p>
<h2 id="gradiente-y-jacobiano"><strong>Gradiente y jacobiano</strong></h2>
<p>En matemáticas, el gradiente es una generalización de la derivada, ya que la derivada de una función esta definida para unicamente funciones de una variable. El gradiente es una función de valor vectorial. El gradiente esta definido como:
$$
\large
\nabla_f(x) \overset{def}{=} \left(
\begin{matrix}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\frac{\partial f(x)}{\partial x_3} \\
\vdots \\
\frac{\partial f(x)}{\partial x_n}
\end{matrix}
\right)
$$
Con ello, se puede observar que el operador gradiente lleva a una función <span class="arithmatex">\(n\)</span> variabas variables a un vector de dimensión <span class="arithmatex">\(n\)</span> donde el elemento i-esimo es la derivada parcial de la función con respecto a la variable <span class="arithmatex">\(i\)</span>. El operador del gradiente se escribe como <span class="arithmatex">\(\nabla \{\}\)</span> y el vector gradiente como <span class="arithmatex">\(\nabla_f\)</span>  </p>
<p>Por otro lado, si se tiene una función que mapea de un espacio vectorial de dimensión <span class="arithmatex">\(n\)</span> a un espacio de dimensión <span class="arithmatex">\(m\)</span> <span class="arithmatex">\((f:\mathbb{R}^n \rightarrow \mathbb{R}^m)\)</span>, el concepto de derivada no se encuetra definido con el operador gradiente. Es por ello que se necesita definir el concepto del operador jacobiano. La definición del jacobiano es la siguiente:<br />
Sea <span class="arithmatex">\(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span>, una función cuyas derivadas parciales de primer orden existen en todo <span class="arithmatex">\(\mathbb{R}^n\)</span> y denotemos como <span class="arithmatex">\(f_1,f_2,f_3,\dots,f_m\)</span> a sus componentes escalares. Se define la matriz jacobiana de <span class="arithmatex">\(f\)</span> en un punto <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> como:
$$
\large
J_f(x) = \left(
\begin{matrix}
\frac{\partial f_1(x)}{\partial x_1} &amp;\frac{\partial f_1(x)}{\partial x_2} &amp; \dots &amp;\frac{\partial f_1(x)}{\partial x_n} \\
\frac{\partial f_2(x)}{\partial x_1} &amp;\frac{\partial f_2(x)}{\partial x_2} &amp; \dots &amp;\frac{\partial f_2(x)}{\partial x_n} \\
\frac{\partial f_3(x)}{\partial x_1} &amp;\frac{\partial f_3(x)}{\partial x_2} &amp; \dots &amp;\frac{\partial f_3(x)}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp;\vdots \\
\frac{\partial f_m(x)}{\partial x_1} &amp;\frac{\partial f_m(x)}{\partial x_2} &amp; \dots &amp;\frac{\partial f_m(x)}{\partial x_n} \\
\end{matrix}
\right)
$$</p>
<h2 id="hessiano"><strong>Hessiano</strong></h2>
<p>La matriz hessiana o hessiano de un campo escalar es una matriz cuadrada de dimensión <span class="arithmatex">\(n \times n\)</span> que tiene como elementos las derivadas parciales de segundo orden. La matriz hessiana se define como:  </p>
<p>Sea <span class="arithmatex">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, un campo escalar cuyas derivadas parciales de segundo grado existen (clase <span class="arithmatex">\(C^2\)</span>). La matriz hessiana de <span class="arithmatex">\(f\)</span>, denotada por <span class="arithmatex">\(H_f(x)\)</span>, es una matriz cuadrada <span class="arithmatex">\(n \times n\)</span> definida como:
$$
\large
H_f(x) = \left(
\begin{matrix}
\frac{\partial^2 f}{\partial x_1^2} (x) &amp;\frac{\partial^2 f}{\partial x_1 \partial x_2} (x) &amp; \dots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} (x) \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} (x) &amp;\frac{\partial^2 f}{\partial x_2^2} (x) &amp; \dots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} (x) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} (x) &amp;\frac{\partial^2 f}{\partial x_n \partial x_2} (x) &amp; \dots &amp; \frac{\partial^2 f}{\partial x_n^2} (x) \\
\end{matrix}
\right)
$$
Con ello, se afirma que el hessiano es aplicar el operador jacobiano al gradiente de la función <span class="arithmatex">\(f\)</span>.
$$
H_f(x) = J\lbrace \nabla f(x)\rbrace = J\lbrace J_f^T \rbrace
$$
Por otro lado, en algunos casos es más preferible calcular el hessiano por medio del operador gradiente. Para lograr este objetivo se define la operación divergencia el cual es la siguiente:
$$
\nabla \cdot f \overset{def}{=} 1^T\nabla f
$$
Con esto se tiene que el hessiano es calculado como:
$$
\nabla^2 f = \nabla^T \nabla f
$$</p>
<h2 id="convexidad"><strong>Convexidad</strong></h2>
<h3 id="conjunto-convexo"><strong>Conjunto convexo</strong></h3>
<p>Un conjunto $\Omega \subseteq \mathbb{R}^n $ es convexo si
$$
\lambda x +(1-\lambda)y \in \Omega, \forall x,y \in \Omega\;\text{y}\; \lambda\in [0,1]
$$</p>
<h3 id="funcion-convexa"><strong>Función convexa</strong></h3>
<p>Una función <span class="arithmatex">\(f: D\rightarrow\mathbb{R}\)</span> es estrictamente convexa si su dominio <span class="arithmatex">\(D\subset \mathbb{R}^n\)</span> es convexo, y para todo <span class="arithmatex">\(x,y \in D\)</span> para todo <span class="arithmatex">\(\lambda \in [0,1]\)</span>, se verifica
$$
f(\lambda x + (1-\lambda)y) \leq \lambda f(x) (1-\lambda) f(x)
$$
Se dice que una función es convexa cuando no se requiere la igualdad de los terminos. Algunos ejemplos de funciones convexas son:
$$
\begin{aligned}
f(x)&amp;=x^2 &amp; \text{Función cuadratica}\nonumber \\
f(x)&amp;=Ax+b &amp; \text{Funciones afines} \nonumber\\
f(x)&amp;=||x|| &amp; \text{Cualquier norma de un vextor} \nonumber\\
\end{aligned} \nonumber
$$</p>
<h3 id="funciones-convexas-diferenciales"><strong>Funciones convexas diferenciales</strong></h3>
<p>Sea <span class="arithmatex">\(f: D\rightarrow \mathbb{R}\)</span> diferenciable sobre un dominio convexo y abierto <span class="arithmatex">\(D\)</span>. Entonces, <span class="arithmatex">\(f\)</span> es convexa si y solo si
$$
f(x) \geq f(\x')+\nabla f(\x')^T (x-\x')
$$
para todo <span class="arithmatex">\(x,\x' \in D\)</span>.</p>
<h3 id="funciones-convexas-diferenciables-dos-veces"><strong>Funciones convexas diferenciables dos veces</strong></h3>
<p>Sea <span class="arithmatex">\(f: D\rightarrow \mathbb{R}\)</span> de clase <span class="arithmatex">\(C^2\)</span> sobre un dominio convexo y abierto <span class="arithmatex">\(D\)</span>. Entonces, <span class="arithmatex">\(f\)</span> es convexa si y solo si <span class="arithmatex">\(H_f(x)\)</span> es semidefinida positiva para todo <span class="arithmatex">\(x\in D\)</span>. Una matriz semidefinida positiva es una matriz hermitiana <span class="arithmatex">\(M\)</span> cuando se cumple una de las siguientes condiciones:</p>
<ol>
<li>Para todos los vectores no nulos (<span class="arithmatex">\(x\in \mathbb{R}^n\)</span>) tenemos que: $$ x^TMx \geq 0 $$</li>
<li>Todos los eigenvalores de <span class="arithmatex">\(M\)</span> son positivos.</li>
<li>La función $$ &lt; x,y  &gt; = x^TMy $$
define un producto interno en <span class="arithmatex">\(\mathbb{R}^n\)</span>.</li>
<li>Todos los menores principales de <span class="arithmatex">\(M\)</span> son positivos.</li>
</ol>
<h2 id="optimalidad"><strong>Optimalidad</strong></h2>
<p>La optimización convexa trata el problema general de minimizar una función convexa, sobre un conjunto factible también convexo:
$$
\underset{x\in D}{min}\; f(x)
$$
donde <span class="arithmatex">\(f:D\rightarrow \mathbb{R}\)</span> es convexa y $ D\subset \mathbb{R}^n$ es convexo. En un problema convexo no hay distinción entre mínimos globales y locales. Es por ello que existen diferentes clasificaciones dependiendo de las condiciones que cumplan. Estas clasificaciones son la siguientes:</p>
<ul>
<li><em>Óptimo global estricto</em>: $x^* \in \mathbb{R}^n $ es el óptimo global estricto si <span class="arithmatex">\(f(y)&gt;f(x^*), \forall y \in \mathbb{R}^n\)</span>.</li>
<li><em>Óptimo global</em>: $x^* \in \mathbb{R}^n $ es el óptimo global si <span class="arithmatex">\(f(y)\geq f(x^*), \forall y \in \mathbb{R}^n\)</span>.</li>
<li><em>Óptimo Local Estricto</em>: $x^* \in \mathbb{R}^n $ es el óptimo local estricto si <span class="arithmatex">\(f(y)&gt; f(x^*), \forall y \in \Omega\)</span>, donde <span class="arithmatex">\(\Omega\)</span> es una vecindad abierta que contiene a <span class="arithmatex">\(x^*\)</span>.</li>
<li><em>Óptimo Local</em>: <span class="arithmatex">\(x^* \in \mathbb{R}^n\)</span> es el óptimo local estricto si <span class="arithmatex">\(f(y) \geq f(x^*), \forall y \in \Omega\)</span>, donde <span class="arithmatex">\(\Omega\)</span> es una vecindad abierta que contiene a <span class="arithmatex">\(x^*\)</span>.</li>
</ul>
<h3 id="minimos-globales-bajo-convexidad"><strong>Minimos globales bajo convexidad</strong></h3>
<p>Considerando el problema de optimización donde se supone que la función <span class="arithmatex">\(f\)</span> es diferenciable. Un punto <span class="arithmatex">\(x^* \in D\)</span> es mínimo global si y solo si
$$
\nabla f(x^*)^T (x-x^*) \geq 0 \;\;\; \forall x \in D
$$</p>
<h3 id="direccion-de-descenso"><strong>Dirección de descenso</strong></h3>
<p>Se dice que <span class="arithmatex">\(d\)</span> es una dirección de descenso en el punto <span class="arithmatex">\(x'\)</span> para el problema propuesto de optimización cuando se cumple que:
$$
f(x') &gt; f(x'+\epsilon d)
$$
donde <span class="arithmatex">\(\epsilon\)</span> es suficientemente pequeña. Con el fin de encontrar la condición de descenso se utilizara una expansión de la serie de Taylor de la función <span class="arithmatex">\(f\)</span> en el punto <span class="arithmatex">\(x'\)</span>. Del procedimiento se obtiene lo siguiente:
$$
\begin{aligned}
f(x') &amp; &gt; f(x'+\epsilon d)\nonumber \\
&amp; &gt; f(x') + \epsilon \nabla f(x')^T d + \epsilon^2 d^T \nabla^2 f(x') d+ \dots \nonumber \\
&amp; \approx f(x') + \epsilon \nabla f(x')^T d \nonumber \\
\end{aligned}
$$
dado que <span class="arithmatex">\(|\epsilon^n| &gt; |epsilon|\)</span> para <span class="arithmatex">\(n&gt;1\)</span>. La dirección de descenso <span class="arithmatex">\(d\)</span> debe cumplir que:
$$
-\nabla f(x')^T d &gt; 0
$$</p>
<h3 id="condicion-de-optimalidad-de-primer-orden"><strong>Condición de optimalidad de primer orden</strong></h3>
<p>Se tiene que en el punto óptimo <span class="arithmatex">\((x^*)\)</span> de una función suave <span class="arithmatex">\((f)\)</span> no existe una dirección de descenso. Esto es:
$$
\nexists d: -\nabla f(x^*)^T d &gt; 0
$$</p>
<p>ya que</p>
<div class="arithmatex">\[
\nabla f(x^*) = 0
\]</div>
<h3 id="condicion-de-optimalidad-de-segundo-orden"><strong>Condición de optimalidad de segundo orden</strong></h3>
<p>En general, la condición de optimalidad de primer orden es necesaria, pero no es suficiente para encontrar una solución. Es decir, en un mínimo local, el gradiente se desvanece. Sin embargo, esto también sucedee cuando se encuentra un máximo local o global. Para distinguir estos casos necesitamos la condiciones de segundo orden.<br />
Sea el problema de optimización sin restricciones:
$$
\underset{x\in D}{min}\; f(x)
$$
La solución <span class="arithmatex">\((x^*)\)</span> satisface la condición de optimalidad primer orden:
$$
\nabla f(x^*) = 0
$$</p>
<p>Esto evita que un método de descenso actualice el punto óptimo:</p>
<div class="arithmatex">\[
x' \leftarrow x' - \alpha \nabla f(x^*)
\]</div>
<p>Sin embargo, por medio la approximación de series de Taylor de segundo orden notamos que para un <span class="arithmatex">\(\epsilon\)</span> suficientemente pequeña:</p>
<div class="arithmatex">\[
f(x^*+\epsilon d)  = f(x^*)+\frac{1}{2}\epsilon^2 d^T \nabla^2 f(x^*) d
\]</div>
<p>como <span class="arithmatex">\(x^*\)</span> es el punto óptimo y <span class="arithmatex">\(\epsilon&gt;0\)</span> se obtiene que:</p>
<div class="arithmatex">\[
d^T \nabla^2 f(x^*) d \geq 0, \forall d
\]</div>
<h3 id="condicion-suficiente-de-segundo-orden"><strong>Condición suficiente de segundo orden</strong></h3>
<p>Sea el problema:
$$
\underset{x\in D}{min}\; f(x)
$$</p>
<p>con <span class="arithmatex">\(f\)</span> suave y <span class="arithmatex">\(x^*\)</span> es un punto tal que:</p>
<div class="arithmatex">\[
\begin{aligned}
\nonumber
\nabla f(x^*) &amp;= 0 \\ \nonumber
\nabla^2 f(x^*) &amp; \;\text{es definido positivo} \\
\end{aligned}
\]</div>
<p>Con el fin de demostrar que <span class="arithmatex">\(x^*\)</span> es un punto óptimo se elije estudiarlo bajo la perspectiva que este punto se encuentra dentro de una bola abierta de radio <span class="arithmatex">\(r\)</span>. esto es:
$$
D = {z:||x^*-x||&lt;0}
$$</p>
<p>tal que <span class="arithmatex">\(\nabla^2 f(x)\)</span> se mantenga definido positivo. Tomando un vector <span class="arithmatex">\(d\)</span> tal que:</p>
<div class="arithmatex">\[
x+\epsilon d \in D
\]</div>
<p>Entonces, tomando un <span class="arithmatex">\(\epsilon\)</span> sumamente pequeño, se tiene que:</p>
<div class="arithmatex">\[
f(x^*+\epsilon d)  = f(x^*)+\frac{1}{2}\epsilon^2 d^T \nabla^2 f(x^*) d
\]</div>
<p>dado que:</p>
<div class="arithmatex">\[
\begin{aligned}
\nonumber
\nabla f(x^*) = 0 \\ \nonumber
\epsilon^2d^T\nabla^2 f(x^*)d &gt; 0 \\
\end{aligned}
\]</div>
<p>tenemos que</p>
<div class="arithmatex">\[
f(x^*+\epsilon d) &gt; f(x^*)
\]</div>
<p>por lo tanto <span class="arithmatex">\(x^*\)</span> es un mínimo de la función.</p>
<p>Como ejemplo de lo antes mencionado tenemos las siguientes graficas generadas por el script <code>optimalidad_example.py</code>. En el caso del primer y segundo ejemplo, las soluciones obtenidas al resolver el gradiente de la función corresponden a mínimos locales. Sin embargo, la diferencia entre estos dos casos es el valor del hessiano en la solución, la cual nos indica la concavidad de la función.</p>
<p>Por otro lado, el ejemplo tres muestra que la solución al problema de optimización corresponde a un punto de mesa. Lo cual no corresponde a una solución aceptable para los propositos de la optimización de funciones.</p>
<p>Para evitar este tipo de soluciones es necesario aplicar la condición suficiente de segundo orden, ya que se supone que el hessiano de la función es positivo definido en la solución. Por lo que, concluimos que <span class="arithmatex">\(x^*\)</span> es óptimo si y solo si <span class="arithmatex">\(\nabla f(x^*)=0, \; \nabla^2 f(x^*) \succeq 0\)</span>.</p>
<p><img alt="optimalidad example" src="../graphics/optimalidad_example.png" title="Optimalidad example" /></p>
<h4 id="optimalidad_examplepy">optimalidad_example.py</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">concatenate</span><span class="p">,</span>
    <span class="n">arange</span><span class="p">,</span>
    <span class="n">zeros</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">z</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">f2</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="o">-</span><span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">z</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">f3</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">x</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">,</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">f3</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7209b7&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
    <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">nabla f=0$</span><span class="se">\n</span><span class="s1">$</span><span class="se">\\</span><span class="s1">nabla^2f </span><span class="se">\\</span><span class="s1">succ 0$&#39;</span><span class="p">,</span>
    <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">7.4</span><span class="p">,</span> <span class="mf">.2</span><span class="p">),</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#4cc9f0&#39;</span><span class="p">,</span>
        <span class="n">shrink</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span>
    <span class="s2">&quot;off&quot;</span>
<span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">f1</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7209b7&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
    <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">nabla f=0$</span><span class="se">\n</span><span class="s1">$</span><span class="se">\\</span><span class="s1">nabla^2 f = 0$&#39;</span><span class="p">,</span>
    <span class="n">xy</span><span class="o">=</span><span class="p">(</span>
        <span class="mf">11.5</span><span class="p">,</span>
        <span class="mi">0</span>
    <span class="p">),</span>
    <span class="n">xytext</span><span class="o">=</span><span class="p">(</span>
        <span class="mf">8.7</span><span class="p">,</span>
        <span class="mf">0.2</span>
    <span class="p">),</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#4cc9f0&#39;</span><span class="p">,</span>
        <span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span>
    <span class="s2">&quot;off&quot;</span>
<span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">f2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#7209b7&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
    <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">nabla f=0$</span><span class="se">\n</span><span class="s1">$</span><span class="se">\\</span><span class="s1">nabla^2f = 0$&#39;</span><span class="p">,</span>
    <span class="n">xy</span><span class="o">=</span><span class="p">(</span>
        <span class="mi">13</span><span class="p">,</span>
        <span class="mi">0</span>
    <span class="p">),</span>
    <span class="n">xytext</span><span class="o">=</span><span class="p">(</span>
        <span class="mf">10.5</span><span class="p">,</span>
        <span class="mf">.3</span>
    <span class="p">),</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#4cc9f0&#39;</span><span class="p">,</span>
        <span class="n">shrink</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span>
    <span class="s2">&quot;off&quot;</span>
<span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span>
    <span class="s2">&quot;test.png&quot;</span>
<span class="p">)</span>
</code></pre></div>

<p>De los tres casos mostrados en la figura anterior, los dos promeros (de la izquierda) corresponden a mínimos locales (soluciones) y el tercero a un punto de mesa (no-solución).</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.expand", "navigation.top", "toc.integrate"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="../javascripts/katex.min.js"></script>
      
        <script src="../javascripts/auto-render.min.js"></script>
      
    
  </body>
</html>